\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{titlesec}

\titleformat{\section}{\Large\bfseries}{}{0em}{}
\titlespacing{\section}{0pt}{1em}{0.5em}

\begin{document}

\section*{Q \& A Preparation}

\noindent
\textbf{Question:} Why did Logistic Regression outperform LSTM?\\
\textbf{Answer:} Classical models like Logistic Regression perform well with TF–IDF features on linear problems. LSTM requires more tuning, data, and often pretrained embeddings to match this performance.

\vspace{1em}

\noindent
\textbf{Question:} Why did the CNN perform better than the LSTM?\\
\textbf{Answer:} CNNs capture local n-gram patterns through convolutions and are easier to train. LSTMs rely on sequential dependencies and struggled here due to underfitting.

\vspace{1em}

\noindent
\textbf{Question:} How did you choose 200 as the sequence length?\\
\textbf{Answer:} It covers most review lengths without incurring high memory costs. We chose 200 as a balance between performance and efficiency.

\vspace{1em}

\noindent
\textbf{Question:} What would improve the LSTM performance?\\
\textbf{Answer:} Using pretrained embeddings like GloVe, adding more layers, regularization, or applying attention mechanisms could significantly help.

\vspace{1em}

\noindent
\textbf{Question:} Would using BERT improve results?\\
\textbf{Answer:} Likely yes. BERT captures deep contextual semantics and typically outperforms LSTM/CNN models on sentiment analysis, at the cost of higher computational demand.

\vspace{1em}

\noindent
\textbf{Question:} What kind of reviews did models fail on?\\
\textbf{Answer:} Sarcastic, very short, or sentiment-ambiguous reviews were the most challenging for all models.

\vspace{1em}

\noindent
\textbf{Question:} How fast is the model inference?\\
\textbf{Answer:} Naïve Bayes is fastest (0.13ms/sample), followed by Logistic Regression (0.47ms), then Linear SVM and CNN. LSTM was slow and ineffective.

\end{document}

